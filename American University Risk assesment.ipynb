{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler as sc\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score,roc_curve,precision_score,recall_score,f1_score,plot_confusion_matrix,classification_report,ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-HjemZK8Q5P",
    "outputId": "c0d013b3-8b3d-40c7-ad40-62391816ac93"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRD_PTS_PER_UNIT</th>\n",
       "      <th>At_Risk</th>\n",
       "      <th>CATALOG_NBR</th>\n",
       "      <th>GPAO</th>\n",
       "      <th>ANON_INSTR_ID</th>\n",
       "      <th>TERM</th>\n",
       "      <th>HSGPA</th>\n",
       "      <th>LAST_ACT_ENGL_SCORE</th>\n",
       "      <th>LAST_ACT_MATH_SCORE</th>\n",
       "      <th>LAST_ACT_READ_SCORE</th>\n",
       "      <th>LAST_ACT_SCIRE_SCORE</th>\n",
       "      <th>LAST_ACT_COMP_SCORE</th>\n",
       "      <th>SEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>F</td>\n",
       "      <td>271</td>\n",
       "      <td>2.945000</td>\n",
       "      <td>3057</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.7</td>\n",
       "      <td>F</td>\n",
       "      <td>271</td>\n",
       "      <td>3.118750</td>\n",
       "      <td>1996</td>\n",
       "      <td>102</td>\n",
       "      <td>3.9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>T</td>\n",
       "      <td>271</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>4230</td>\n",
       "      <td>102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>F</td>\n",
       "      <td>271</td>\n",
       "      <td>3.738462</td>\n",
       "      <td>56</td>\n",
       "      <td>102</td>\n",
       "      <td>3.7</td>\n",
       "      <td>25.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.4</td>\n",
       "      <td>F</td>\n",
       "      <td>271</td>\n",
       "      <td>3.326531</td>\n",
       "      <td>283</td>\n",
       "      <td>102</td>\n",
       "      <td>3.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>3.3</td>\n",
       "      <td>F</td>\n",
       "      <td>271</td>\n",
       "      <td>3.760000</td>\n",
       "      <td>2458</td>\n",
       "      <td>122</td>\n",
       "      <td>4.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>4.0</td>\n",
       "      <td>F</td>\n",
       "      <td>271</td>\n",
       "      <td>4.209091</td>\n",
       "      <td>779</td>\n",
       "      <td>122</td>\n",
       "      <td>3.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>3.0</td>\n",
       "      <td>F</td>\n",
       "      <td>271</td>\n",
       "      <td>3.231707</td>\n",
       "      <td>2458</td>\n",
       "      <td>122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>2.7</td>\n",
       "      <td>F</td>\n",
       "      <td>271</td>\n",
       "      <td>3.384615</td>\n",
       "      <td>2886</td>\n",
       "      <td>122</td>\n",
       "      <td>3.8</td>\n",
       "      <td>33.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>2.0</td>\n",
       "      <td>F</td>\n",
       "      <td>271</td>\n",
       "      <td>3.660870</td>\n",
       "      <td>779</td>\n",
       "      <td>122</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1862 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GRD_PTS_PER_UNIT At_Risk  CATALOG_NBR      GPAO  ANON_INSTR_ID  TERM  \\\n",
       "0                  2.0       F          271  2.945000           3057   102   \n",
       "1                  1.7       F          271  3.118750           1996   102   \n",
       "2                  0.0       T          271  2.500000           4230   102   \n",
       "3                  2.0       F          271  3.738462             56   102   \n",
       "4                  2.4       F          271  3.326531            283   102   \n",
       "...                ...     ...          ...       ...            ...   ...   \n",
       "1857               3.3       F          271  3.760000           2458   122   \n",
       "1858               4.0       F          271  4.209091            779   122   \n",
       "1859               3.0       F          271  3.231707           2458   122   \n",
       "1860               2.7       F          271  3.384615           2886   122   \n",
       "1861               2.0       F          271  3.660870            779   122   \n",
       "\n",
       "      HSGPA  LAST_ACT_ENGL_SCORE  LAST_ACT_MATH_SCORE  LAST_ACT_READ_SCORE  \\\n",
       "0       NaN                  NaN                  NaN                  NaN   \n",
       "1       3.9                 29.0                 26.0                 34.0   \n",
       "2       0.0                 25.0                 30.0                  NaN   \n",
       "3       3.7                 25.0                 34.0                  NaN   \n",
       "4       3.6                  NaN                  NaN                  NaN   \n",
       "...     ...                  ...                  ...                  ...   \n",
       "1857    4.0                 33.0                 34.0                 32.0   \n",
       "1858    3.8                  NaN                  NaN                  NaN   \n",
       "1859    0.0                  NaN                  NaN                  NaN   \n",
       "1860    3.8                 33.0                 25.0                 27.0   \n",
       "1861    4.0                  NaN                  NaN                  NaN   \n",
       "\n",
       "      LAST_ACT_SCIRE_SCORE  LAST_ACT_COMP_SCORE SEX  \n",
       "0                      NaN                  NaN   F  \n",
       "1                     32.0                 31.0   F  \n",
       "2                      NaN                 30.0   F  \n",
       "3                      NaN                 32.0   F  \n",
       "4                      NaN                  NaN   M  \n",
       "...                    ...                  ...  ..  \n",
       "1857                  26.0                 31.0   M  \n",
       "1858                   NaN                  NaN   F  \n",
       "1859                   NaN                  NaN   F  \n",
       "1860                  26.0                 28.0   F  \n",
       "1861                   NaN                  NaN   F  \n",
       "\n",
       "[1862 rows x 13 columns]"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"USA_Uni_Subject .csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRnmKXRLBeew"
   },
   "source": [
    "# **i. If not, state the reason and write a Python script to perform any necessary preprocessing so that the data becomes suitable to be used.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAauwJuSgCeD"
   },
   "source": [
    "### Lets Replace dummy values \"?\" and numerical values in SEX column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "id": "qCA1yJ30FVq0"
   },
   "outputs": [],
   "source": [
    "df.loc[(df[\"SEX\"]!=\"M\") & (df[\"SEX\"]!=\"F\"),\"SEX\"]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "awIC6VQR-RjO",
    "outputId": "d0e3a53f-613c-4c8e-de22-34d5385f5f4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colum HSGPA Has 24.06015037593985 Percent Null values\n",
      "Colum LAST_ACT_ENGL_SCORE Has 53.75939849624061 Percent Null values\n",
      "Colum LAST_ACT_MATH_SCORE Has 53.92051557465092 Percent Null values\n",
      "Colum LAST_ACT_READ_SCORE Has 59.66702470461869 Percent Null values\n",
      "Colum LAST_ACT_SCIRE_SCORE Has 59.828141783029 Percent Null values\n",
      "Colum LAST_ACT_COMP_SCORE Has 53.75939849624061 Percent Null values\n",
      "Colum SEX Has 16.487647690655212 Percent Null values\n"
     ]
    }
   ],
   "source": [
    "#Check Null value percentage\n",
    "\n",
    "#foor loop for iterating through column name and null percent values\n",
    "for cname,nperc in zip(((df.isna().sum()/df.shape[0])*100).index,(df.isna().sum()/df.shape[0])*100):\n",
    "  #skip columns with no null values\n",
    "  if nperc>0:\n",
    "    print(\"Colum\",cname,\"Has\",nperc,\"Percent Null values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC8rdAMeKYdv"
   },
   "source": [
    "1.   We observe that HSGPA and Sex columns have less than 30% missing values,so we will try to impute them with appropriate method but we will have to drop null values in SEX column because it makes no meaning to impure Gender based on assumption.\n",
    "2.   While Other columns having more than 30% missing values will be deleted as there is no point in imputing them and imputing them can cause a bias in our dataset for ML modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "id": "gAsxImn6K80B"
   },
   "outputs": [],
   "source": [
    "#Dropping columns with more than 30% missing values and Null values in sex because imputing sex based on assumption doesnt sound correct.\n",
    "#It may happen that there might be some person at risk of failure and is a M and we impute it as F,will be wrong.\n",
    "\n",
    "#dropping nulls above 30% and CATALOG_NBR because we observe that the columns consists of one value 271,so for all the records it doesnt change so we drop that as it might create a bias in dataset\n",
    "df.drop([\"CATALOG_NBR\",\"LAST_ACT_ENGL_SCORE\",\"LAST_ACT_MATH_SCORE\",\"LAST_ACT_READ_SCORE\",\"LAST_ACT_SCIRE_SCORE\",\"LAST_ACT_COMP_SCORE\"],axis=1,inplace=True)\n",
    "\n",
    "#dropping sex with null values\n",
    "df.drop(df[df[\"SEX\"].isna()].index.to_list(),axis=0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check the mean and median differnece to see if to use mean or median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5333682514101414 0.6332947976878627\n"
     ]
    }
   ],
   "source": [
    "diff_F=df.loc[(df[\"At_Risk\"]==\"F\") & (df[\"HSGPA\"].isna()==False),\"HSGPA\"].mean()-df.loc[(df[\"At_Risk\"]==\"F\") & (df[\"HSGPA\"].isna()==False),\"HSGPA\"].median()\n",
    "diff_T=df.loc[(df[\"At_Risk\"]==\"T\") & (df[\"HSGPA\"].isna()==False),\"HSGPA\"].mean()-df.loc[(df[\"At_Risk\"]==\"T\") & (df[\"HSGPA\"].isna()==False),\"HSGPA\"].median()\n",
    "print(abs(diff_F),abs(diff_T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From above result we observe that the mean and median difference is very less so we can use mean or median for imputing the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vI3zn6L7LjJi",
    "outputId": "cf120c05-fb54-4a4c-8693-a4018637a2b0"
   },
   "outputs": [],
   "source": [
    "#imputing HSGPA based on At_Risk cols using median\n",
    "median_F=df.loc[(df[\"At_Risk\"]==\"F\"),\"HSGPA\"].median()\n",
    "median_T=df.loc[(df[\"At_Risk\"]==\"T\"),\"HSGPA\"].median()\n",
    "df.loc[(df[\"At_Risk\"]==\"F\") & (df[\"HSGPA\"].isna()),\"HSGPA\"]=median_F\n",
    "df.loc[(df[\"At_Risk\"]==\"T\") & (df[\"HSGPA\"].isna()),\"HSGPA\"]=median_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7LRKFNhfmD7"
   },
   "source": [
    "### Checking null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGYogr4_eUcV",
    "outputId": "c5819d27-3c0c-4baa-ff30-a0f79bc577f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRD_PTS_PER_UNIT    0\n",
      "At_Risk             0\n",
      "GPAO                0\n",
      "ANON_INSTR_ID       0\n",
      "TERM                0\n",
      "HSGPA               0\n",
      "SEX                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking null values\n",
    "\n",
    "#We observe that all the necesaary null values have now been imputed\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iii. Submit the pre-processed data in CSV format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:\\\\Users\\\\Angat\\\\Downloads\\\\preprocessed_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Create a NN Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "def attributeCombinations(sample_list):\n",
    "    #Declares a list for storing column combinations to be tested\n",
    "    newList=[]\n",
    "    k=[list(combinations(sample_list,i)) for i in range(1,len(sample_list)+1)]\n",
    "    for j in k:\n",
    "        for m in j:\n",
    "            newList.append(list(m))\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before splitting lets transform Sex and At_risk into binary format\n",
    "df[\"At_Risk\"]=df[\"At_Risk\"].apply(lambda x:1 if x==\"T\" else 0)\n",
    "df[\"SEX\"]=df[\"SEX\"].apply(lambda x:1 if x==\"M\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:\\\\Users\\\\Angat\\\\Downloads\\\\preprocessed_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We drop ANON_INSTR_ID because having any instructor to mark wont make a difference unless we are assuming a particular instructor is strict to eliminate.\n",
    "#Similarly assuming for Term it doesnt matter which term the student is from,he can be at risk in any term i.e lower or higher.\n",
    "X=df.drop([\"At_Risk\"],axis=1)\n",
    "Y=df[\"At_Risk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Angat\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angat\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angat\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angat\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angat\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angat\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angat\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angat\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angat\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angat\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angat\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#After Scaling the dataset,we now look for columns that best fit the data.\n",
    "mods=pd.DataFrame({})\n",
    "for i in attributeCombinations(X.columns.to_list()):\n",
    "    X_train,X_test,Y_train,Y_test=tts(X[i],Y,test_size=0.25,random_state=1)\n",
    "    model.fit(X_train,Y_train)\n",
    "    mods=mods.append({\"Cols\":i,\"Training\":accuracy_score(model.predict(X_train),Y_train),\"Testing\":accuracy_score(model.predict(X_test),Y_test),\"Length\":len(i)},ignore_index=True)\n",
    "mods[\"Cols\"]=mods[\"Cols\"].apply(lambda x:\",\".join(x))\n",
    "mods.sort_values([\"Training\",\"Testing\"],ascending=False)\n",
    "#from aboove we see that GRD_PTS_PER_UNIT,GPAO fit best so we use these two columns\n",
    "X.drop(['ANON_INSTR_ID', 'TERM'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=tts(X,Y,test_size=0.25,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy Before Scaling : 1.0\n"
     ]
    }
   ],
   "source": [
    "model=MLPClassifier(random_state=2)\n",
    "model.fit(X_train,Y_train)\n",
    "print(\"Training Accuracy Before Scaling :\",accuracy_score(model.predict(X_train),Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy Before Scaling : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Accuracy Before Scaling :\",accuracy_score(model.predict(X_test),Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From above we observe that,the model is performing good on training and testing dataset and there isnt much difference in training and testing accuracy score that points overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets train the same model when we standard scale the traing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We remove SEX columns because that a binary columns and we dont want to include it in standard scaling\n",
    "X=X.drop([\"SEX\"],axis=1)\n",
    "std=sc().fit(X)\n",
    "X_transformed=pd.DataFrame(std.transform(X),columns=X.columns)\n",
    "#Now that we have scaled the variables,we can include SEX colum\n",
    "X_transformed[\"SEX\"]=df[\"SEX\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #After Scaling the dataset,we now look for columns that best fit the data.\n",
    "# mods=pd.DataFrame({})\n",
    "# for i in attributeCombinations(X_transformed.columns.to_list()):\n",
    "#     X_train,X_test,Y_train,Y_test=tts(X_transformed[i],Y,test_size=0.25,random_state=1)\n",
    "#     model.fit(X_train,Y_train)\n",
    "#     mods=mods.append({\"Cols\":i,\"Training\":accuracy_score(model.predict(X_train),Y_train),\"Testing\":accuracy_score(model.predict(X_test),Y_test),\"Length\":len(i)},ignore_index=True)\n",
    "# mods[\"Cols\"]=mods[\"Cols\"].apply(lambda x:\",\".join(x))\n",
    "# mods.sort_values([\"Training\",\"Testing\"],ascending=False)\n",
    "# #from aboove we see that GRD_PTS_PER_UNIT,GPAO fit best so we use these two columns\n",
    "# X_transformed.drop(['ANON_INSTR_ID', 'TERM', 'HSGPA', 'SEX'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRD_PTS_PER_UNIT</th>\n",
       "      <th>GPAO</th>\n",
       "      <th>HSGPA</th>\n",
       "      <th>SEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.520086</td>\n",
       "      <td>-0.932839</td>\n",
       "      <td>0.381831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.776664</td>\n",
       "      <td>-0.563052</td>\n",
       "      <td>0.537281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.230606</td>\n",
       "      <td>-1.879919</td>\n",
       "      <td>-2.493986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.520086</td>\n",
       "      <td>0.755861</td>\n",
       "      <td>0.381831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.177982</td>\n",
       "      <td>-0.120839</td>\n",
       "      <td>0.304106</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>0.591752</td>\n",
       "      <td>0.801700</td>\n",
       "      <td>0.615006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>1.190434</td>\n",
       "      <td>1.757487</td>\n",
       "      <td>0.459556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>0.335174</td>\n",
       "      <td>-0.322649</td>\n",
       "      <td>-2.493986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>0.078596</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.459556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>-0.520086</td>\n",
       "      <td>0.590724</td>\n",
       "      <td>0.615006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1555 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GRD_PTS_PER_UNIT      GPAO     HSGPA  SEX\n",
       "0            -0.520086 -0.932839  0.381831    0\n",
       "1            -0.776664 -0.563052  0.537281    0\n",
       "2            -2.230606 -1.879919 -2.493986    0\n",
       "3            -0.520086  0.755861  0.381831    0\n",
       "4            -0.177982 -0.120839  0.304106    1\n",
       "...                ...       ...       ...  ...\n",
       "1550          0.591752  0.801700  0.615006    1\n",
       "1551          1.190434  1.757487  0.459556    0\n",
       "1552          0.335174 -0.322649 -2.493986    0\n",
       "1553          0.078596  0.002781  0.459556    0\n",
       "1554         -0.520086  0.590724  0.615006    0\n",
       "\n",
       "[1555 rows x 4 columns]"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.9991423670668954\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,Y_train,Y_test=tts(X_transformed,Y,test_size=0.25,random_state=1)\n",
    "model.fit(X_train,Y_train)\n",
    "print(\"Training Accuracy :\",accuracy_score(model.predict(X_train),Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.9974293059125964\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy :\",accuracy_score(model.predict(X_test),Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From above we observe that,the model is performing better after standard scaling on training.And Training and testing accuracies on dataset dont have much difference in accuracy score that points overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using paramter tuning lets find the best paramters to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the set of parameters for MLP classifiers to pick\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(100,),(150,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001,0.01],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decalring a Grid Search method to iterate over the parameters and apply to classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=MLPClassifier(random_state=2), n_jobs=-1,\n",
       "             param_grid={'activation': ['tanh', 'relu'],\n",
       "                         'alpha': [0.0001, 0.001, 0.01],\n",
       "                         'hidden_layer_sizes': [(100,), (150,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['sgd', 'adam']})"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(model, parameter_space, n_jobs=-1, cv=10)\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets now see the performances of each fols with paramter values considered for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.997429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.997429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.997429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.997429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.997429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>adam</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.997429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.989736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.989736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.989736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.989736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.989736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.989736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.989729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.989729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.989729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.989729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.989729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.989729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.988882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.988882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.988882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.988882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.988882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.988882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.988874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.988874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.988874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.988874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>constant</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.988874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(150,)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.988874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_activation param_alpha param_hidden_layer_sizes param_learning_rate  \\\n",
       "23             tanh        0.01                   (150,)            adaptive   \n",
       "15             tanh       0.001                   (150,)            adaptive   \n",
       "21             tanh        0.01                   (150,)            constant   \n",
       "3              tanh      0.0001                   (100,)            adaptive   \n",
       "19             tanh        0.01                   (100,)            adaptive   \n",
       "5              tanh      0.0001                   (150,)            constant   \n",
       "7              tanh      0.0001                   (150,)            adaptive   \n",
       "17             tanh        0.01                   (100,)            constant   \n",
       "9              tanh       0.001                   (100,)            constant   \n",
       "11             tanh       0.001                   (100,)            adaptive   \n",
       "1              tanh      0.0001                   (100,)            constant   \n",
       "13             tanh       0.001                   (150,)            constant   \n",
       "29             relu      0.0001                   (150,)            constant   \n",
       "31             relu      0.0001                   (150,)            adaptive   \n",
       "47             relu        0.01                   (150,)            adaptive   \n",
       "37             relu       0.001                   (150,)            constant   \n",
       "39             relu       0.001                   (150,)            adaptive   \n",
       "45             relu        0.01                   (150,)            constant   \n",
       "33             relu       0.001                   (100,)            constant   \n",
       "41             relu        0.01                   (100,)            constant   \n",
       "43             relu        0.01                   (100,)            adaptive   \n",
       "27             relu      0.0001                   (100,)            adaptive   \n",
       "25             relu      0.0001                   (100,)            constant   \n",
       "35             relu       0.001                   (100,)            adaptive   \n",
       "0              tanh      0.0001                   (100,)            constant   \n",
       "8              tanh       0.001                   (100,)            constant   \n",
       "2              tanh      0.0001                   (100,)            adaptive   \n",
       "18             tanh        0.01                   (100,)            adaptive   \n",
       "16             tanh        0.01                   (100,)            constant   \n",
       "10             tanh       0.001                   (100,)            adaptive   \n",
       "24             relu      0.0001                   (100,)            constant   \n",
       "32             relu       0.001                   (100,)            constant   \n",
       "42             relu        0.01                   (100,)            adaptive   \n",
       "34             relu       0.001                   (100,)            adaptive   \n",
       "40             relu        0.01                   (100,)            constant   \n",
       "26             relu      0.0001                   (100,)            adaptive   \n",
       "46             relu        0.01                   (150,)            adaptive   \n",
       "44             relu        0.01                   (150,)            constant   \n",
       "36             relu       0.001                   (150,)            constant   \n",
       "38             relu       0.001                   (150,)            adaptive   \n",
       "30             relu      0.0001                   (150,)            adaptive   \n",
       "28             relu      0.0001                   (150,)            constant   \n",
       "12             tanh       0.001                   (150,)            constant   \n",
       "6              tanh      0.0001                   (150,)            adaptive   \n",
       "14             tanh       0.001                   (150,)            adaptive   \n",
       "4              tanh      0.0001                   (150,)            constant   \n",
       "20             tanh        0.01                   (150,)            constant   \n",
       "22             tanh        0.01                   (150,)            adaptive   \n",
       "\n",
       "   param_solver  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "23         adam           1.000000           1.000000           1.000000   \n",
       "15         adam           1.000000           1.000000           1.000000   \n",
       "21         adam           1.000000           1.000000           1.000000   \n",
       "3          adam           1.000000           1.000000           1.000000   \n",
       "19         adam           1.000000           1.000000           1.000000   \n",
       "5          adam           1.000000           1.000000           1.000000   \n",
       "7          adam           1.000000           1.000000           1.000000   \n",
       "17         adam           1.000000           1.000000           1.000000   \n",
       "9          adam           1.000000           1.000000           1.000000   \n",
       "11         adam           1.000000           1.000000           1.000000   \n",
       "1          adam           1.000000           1.000000           1.000000   \n",
       "13         adam           1.000000           1.000000           1.000000   \n",
       "29         adam           1.000000           1.000000           0.991453   \n",
       "31         adam           1.000000           1.000000           0.991453   \n",
       "47         adam           1.000000           1.000000           0.991453   \n",
       "37         adam           1.000000           1.000000           0.991453   \n",
       "39         adam           1.000000           1.000000           0.991453   \n",
       "45         adam           1.000000           1.000000           0.991453   \n",
       "33         adam           1.000000           1.000000           0.991453   \n",
       "41         adam           1.000000           1.000000           0.991453   \n",
       "43         adam           1.000000           1.000000           0.991453   \n",
       "27         adam           1.000000           1.000000           0.991453   \n",
       "25         adam           1.000000           1.000000           0.991453   \n",
       "35         adam           1.000000           1.000000           0.991453   \n",
       "0           sgd           0.974359           0.974359           0.982906   \n",
       "8           sgd           0.974359           0.974359           0.982906   \n",
       "2           sgd           0.974359           0.974359           0.982906   \n",
       "18          sgd           0.974359           0.974359           0.982906   \n",
       "16          sgd           0.974359           0.974359           0.982906   \n",
       "10          sgd           0.974359           0.974359           0.982906   \n",
       "24          sgd           0.982906           0.982906           0.974359   \n",
       "32          sgd           0.982906           0.982906           0.974359   \n",
       "42          sgd           0.982906           0.982906           0.974359   \n",
       "34          sgd           0.982906           0.982906           0.974359   \n",
       "40          sgd           0.982906           0.982906           0.974359   \n",
       "26          sgd           0.982906           0.982906           0.974359   \n",
       "46          sgd           0.974359           0.982906           0.974359   \n",
       "44          sgd           0.974359           0.982906           0.974359   \n",
       "36          sgd           0.974359           0.982906           0.974359   \n",
       "38          sgd           0.974359           0.982906           0.974359   \n",
       "30          sgd           0.974359           0.982906           0.974359   \n",
       "28          sgd           0.974359           0.982906           0.974359   \n",
       "12          sgd           0.974359           0.974359           0.982906   \n",
       "6           sgd           0.974359           0.974359           0.982906   \n",
       "14          sgd           0.974359           0.974359           0.982906   \n",
       "4           sgd           0.974359           0.974359           0.982906   \n",
       "20          sgd           0.974359           0.974359           0.982906   \n",
       "22          sgd           0.974359           0.974359           0.982906   \n",
       "\n",
       "    split3_test_score  split4_test_score  split5_test_score  \\\n",
       "23           1.000000                1.0           1.000000   \n",
       "15           1.000000                1.0           1.000000   \n",
       "21           1.000000                1.0           1.000000   \n",
       "3            1.000000                1.0           1.000000   \n",
       "19           1.000000                1.0           1.000000   \n",
       "5            1.000000                1.0           1.000000   \n",
       "7            1.000000                1.0           1.000000   \n",
       "17           1.000000                1.0           1.000000   \n",
       "9            1.000000                1.0           1.000000   \n",
       "11           1.000000                1.0           1.000000   \n",
       "1            1.000000                1.0           1.000000   \n",
       "13           1.000000                1.0           1.000000   \n",
       "29           1.000000                1.0           0.991453   \n",
       "31           1.000000                1.0           0.991453   \n",
       "47           1.000000                1.0           0.991453   \n",
       "37           1.000000                1.0           0.991453   \n",
       "39           1.000000                1.0           0.991453   \n",
       "45           1.000000                1.0           0.991453   \n",
       "33           1.000000                1.0           0.991453   \n",
       "41           1.000000                1.0           0.991453   \n",
       "43           1.000000                1.0           0.991453   \n",
       "27           1.000000                1.0           0.991453   \n",
       "25           1.000000                1.0           0.991453   \n",
       "35           1.000000                1.0           0.991453   \n",
       "0            1.000000                1.0           0.974359   \n",
       "8            1.000000                1.0           0.974359   \n",
       "2            1.000000                1.0           0.974359   \n",
       "18           1.000000                1.0           0.974359   \n",
       "16           1.000000                1.0           0.974359   \n",
       "10           1.000000                1.0           0.974359   \n",
       "24           1.000000                1.0           0.974359   \n",
       "32           1.000000                1.0           0.974359   \n",
       "42           1.000000                1.0           0.974359   \n",
       "34           1.000000                1.0           0.974359   \n",
       "40           1.000000                1.0           0.974359   \n",
       "26           1.000000                1.0           0.974359   \n",
       "46           0.991453                1.0           0.974359   \n",
       "44           0.991453                1.0           0.974359   \n",
       "36           0.991453                1.0           0.974359   \n",
       "38           0.991453                1.0           0.974359   \n",
       "30           0.991453                1.0           0.974359   \n",
       "28           0.991453                1.0           0.974359   \n",
       "12           1.000000                1.0           0.974359   \n",
       "6            1.000000                1.0           0.974359   \n",
       "14           1.000000                1.0           0.974359   \n",
       "4            1.000000                1.0           0.974359   \n",
       "20           1.000000                1.0           0.974359   \n",
       "22           1.000000                1.0           0.974359   \n",
       "\n",
       "    split6_test_score  split7_test_score  split8_test_score  \\\n",
       "23                1.0                1.0                1.0   \n",
       "15                1.0                1.0                1.0   \n",
       "21                1.0                1.0                1.0   \n",
       "3                 1.0                1.0                1.0   \n",
       "19                1.0                1.0                1.0   \n",
       "5                 1.0                1.0                1.0   \n",
       "7                 1.0                1.0                1.0   \n",
       "17                1.0                1.0                1.0   \n",
       "9                 1.0                1.0                1.0   \n",
       "11                1.0                1.0                1.0   \n",
       "1                 1.0                1.0                1.0   \n",
       "13                1.0                1.0                1.0   \n",
       "29                1.0                1.0                1.0   \n",
       "31                1.0                1.0                1.0   \n",
       "47                1.0                1.0                1.0   \n",
       "37                1.0                1.0                1.0   \n",
       "39                1.0                1.0                1.0   \n",
       "45                1.0                1.0                1.0   \n",
       "33                1.0                1.0                1.0   \n",
       "41                1.0                1.0                1.0   \n",
       "43                1.0                1.0                1.0   \n",
       "27                1.0                1.0                1.0   \n",
       "25                1.0                1.0                1.0   \n",
       "35                1.0                1.0                1.0   \n",
       "0                 1.0                1.0                1.0   \n",
       "8                 1.0                1.0                1.0   \n",
       "2                 1.0                1.0                1.0   \n",
       "18                1.0                1.0                1.0   \n",
       "16                1.0                1.0                1.0   \n",
       "10                1.0                1.0                1.0   \n",
       "24                1.0                1.0                1.0   \n",
       "32                1.0                1.0                1.0   \n",
       "42                1.0                1.0                1.0   \n",
       "34                1.0                1.0                1.0   \n",
       "40                1.0                1.0                1.0   \n",
       "26                1.0                1.0                1.0   \n",
       "46                1.0                1.0                1.0   \n",
       "44                1.0                1.0                1.0   \n",
       "36                1.0                1.0                1.0   \n",
       "38                1.0                1.0                1.0   \n",
       "30                1.0                1.0                1.0   \n",
       "28                1.0                1.0                1.0   \n",
       "12                1.0                1.0                1.0   \n",
       "6                 1.0                1.0                1.0   \n",
       "14                1.0                1.0                1.0   \n",
       "4                 1.0                1.0                1.0   \n",
       "20                1.0                1.0                1.0   \n",
       "22                1.0                1.0                1.0   \n",
       "\n",
       "    split9_test_score  mean_test_score  \n",
       "23           1.000000         1.000000  \n",
       "15           1.000000         1.000000  \n",
       "21           1.000000         1.000000  \n",
       "3            1.000000         1.000000  \n",
       "19           1.000000         1.000000  \n",
       "5            1.000000         1.000000  \n",
       "7            1.000000         1.000000  \n",
       "17           1.000000         1.000000  \n",
       "9            1.000000         1.000000  \n",
       "11           1.000000         1.000000  \n",
       "1            1.000000         1.000000  \n",
       "13           1.000000         1.000000  \n",
       "29           1.000000         0.998291  \n",
       "31           1.000000         0.998291  \n",
       "47           1.000000         0.998291  \n",
       "37           1.000000         0.998291  \n",
       "39           1.000000         0.998291  \n",
       "45           1.000000         0.998291  \n",
       "33           0.991379         0.997429  \n",
       "41           0.991379         0.997429  \n",
       "43           0.991379         0.997429  \n",
       "27           0.991379         0.997429  \n",
       "25           0.991379         0.997429  \n",
       "35           0.991379         0.997429  \n",
       "0            0.991379         0.989736  \n",
       "8            0.991379         0.989736  \n",
       "2            0.991379         0.989736  \n",
       "18           0.991379         0.989736  \n",
       "16           0.991379         0.989736  \n",
       "10           0.991379         0.989736  \n",
       "24           0.982759         0.989729  \n",
       "32           0.982759         0.989729  \n",
       "42           0.982759         0.989729  \n",
       "34           0.982759         0.989729  \n",
       "40           0.982759         0.989729  \n",
       "26           0.982759         0.989729  \n",
       "46           0.991379         0.988882  \n",
       "44           0.991379         0.988882  \n",
       "36           0.991379         0.988882  \n",
       "38           0.991379         0.988882  \n",
       "30           0.991379         0.988882  \n",
       "28           0.991379         0.988882  \n",
       "12           0.982759         0.988874  \n",
       "6            0.982759         0.988874  \n",
       "14           0.982759         0.988874  \n",
       "4            0.982759         0.988874  \n",
       "20           0.982759         0.988874  \n",
       "22           0.982759         0.988874  "
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We tabulate all the paramter folds and possible values taken and their scores\n",
    "cv_results = pd.DataFrame(clf.cv_results_)\n",
    "#We drop the params column because we already have individual param values in the dataframe\n",
    "cv_results.drop([\"params\"],axis=1,inplace=True)\n",
    "#We now sort the dataframe by avg test score\n",
    "cv_results.sort_values([\"mean_test_score\"],ascending=False).loc[:,\"param_activation\":\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Classification Model : MLPClassifier(activation='tanh', random_state=2)\n",
      "\n",
      "Best Classification Model parameters:\n",
      "activation=tanh\n",
      "alpha=0.0001\n",
      "hidden_layer_sizes=(100,)\n",
      "learning_rate=constant\n",
      "solver=adam\n"
     ]
    }
   ],
   "source": [
    "#Using the best params and estimator we can get our best classification model and set of parameters that suit well for predictions\n",
    "print(\"Best Classification Model :\",clf.best_estimator_) \n",
    "print(\"\\nBest Classification Model parameters:\") \n",
    "for k in clf.best_params_:\n",
    "    print(f\"{k}={clf.best_params_[k]}\")\n",
    "grid_predictions = clf.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We observe that these paramters have imporoved the perfromance in more better way compared to classifier without tuning.\n",
    "\n",
    "### Lets now check the performance of tuned model paramters and check whether ofr not this overfits or underfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 1.0\n",
      "Testing Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "param_values=clf.best_params_\n",
    "bestClassificationTunedModel=MLPClassifier(activation=param_values[\"activation\"],\n",
    "                         alpha=param_values[\"alpha\"],\n",
    "                         hidden_layer_sizes=param_values[\"hidden_layer_sizes\"],\n",
    "                         learning_rate=param_values[\"learning_rate\"],\n",
    "                         solver=param_values[\"solver\"],\n",
    "                         random_state=2)\n",
    "bestClassificationTunedModel.fit(X_train,Y_train)\n",
    "print(\"Training Accuracy :\",accuracy_score(bestClassificationTunedModel.predict(X_train),Y_train))\n",
    "print(\"Testing Accuracy :\",accuracy_score(bestClassificationTunedModel.predict(X_test),Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We observe that the training and testing accuracies have no difference hence we can state that model is not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lets evaluate the Cost of classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost For Model: -49\n",
      "Model Cost for misclassified points : 0\n"
     ]
    }
   ],
   "source": [
    "#Cost Calculation for model\n",
    "cm=confusion_matrix(bestClassificationTunedModel.predict(X_test),Y_test)\n",
    "cost_matrix = np.array([[0,1], [100,-1]])\n",
    "ModelCost=np.multiply(cm, cost_matrix).sum()\n",
    "print(\"Cost For Model:\",ModelCost)\n",
    "#Calculating cost for wrong classified points\n",
    "print(\"Model Cost for misclassified points :\",ModelCost*(cm[0,1]+cm[1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So the model cost for misclassified points is below 0 because there are no misclassifications observed so we cannot simply rely on cost of model performance and we will have to evaluate model performance using other evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets now evaluate Metrics like accuracy,Sensitivity,Specificity,ROC and loss per curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 1.0\n",
      "Sensitivity Score :  1.0\n",
      "Specificity Score :  1.0\n",
      "ROC_AUC Score : 1.0\n",
      "Precision Score : 1.0\n",
      "Recall Score : 1.0\n",
      "F1 Score : 1.0\n"
     ]
    }
   ],
   "source": [
    "modelPredictions=bestClassificationTunedModel.predict(X_test)\n",
    "\n",
    "#Print accuracy,Sensitivity,Specificity,ROC\n",
    "print(\"Accuracy Score :\",accuracy_score(modelPredictions,Y_test))\n",
    "sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "print('Sensitivity Score : ', sensitivity )\n",
    "specificity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "print('Specificity Score : ', specificity)\n",
    "print(\"ROC_AUC Score :\",roc_auc_score(modelPredictions,Y_test))\n",
    "print(\"Precision Score :\",precision_score(modelPredictions,Y_test))\n",
    "print(\"Recall Score :\",recall_score(modelPredictions,Y_test))\n",
    "print(\"F1 Score :\",f1_score(modelPredictions,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       340\n",
      "           1       1.00      1.00      1.00        49\n",
      "\n",
      "    accuracy                           1.00       389\n",
      "   macro avg       1.00      1.00      1.00       389\n",
      "weighted avg       1.00      1.00      1.00       389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification report \n",
    "print(classification_report(Y_test, modelPredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot Confusion Matrix\n",
    "ConfusionMatrixDisplay(cm, display_labels=['Not At Risk', 'At Risk']).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 750x500 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot model loss curve\n",
    "lossPerIteration = bestClassificationTunedModel.loss_curve_ #model loss curve\n",
    "plt.figure(dpi=125)\n",
    "plt.plot(lossPerIteration,color=\"green\")\n",
    "plt.title(\"Loss curve\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the metrics and loss per curve, we observe that our model is performing good and identifying all the Actual risks correctly and also able to predict cases where student is not at risk accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Angat\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angat\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Angat\\anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.0,2.945,0,1\n",
    "a,b,c,d=1.0,1,0.7,0\n",
    "std.transform(pd.DataFrame([[a,b,c]]))\n",
    "new_array = np.append(std.transform(pd.DataFrame([[a,b,c]])),d).reshape(1,-1)\n",
    "new_array\n",
    "bestClassificationTunedModel.predict(new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open(\"C:\\\\Users\\\\Angat\\\\Downloads\\\\model.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[\"GRD_PTS_PER_UNIT\"].max()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
